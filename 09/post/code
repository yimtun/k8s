func NewProxyCommand() *cobra.Command {
	opts := NewOptions()

	cmd := &cobra.Command{
		Use: "kube-proxy",
		Long: `The Kubernetes network proxy runs on each node. This
reflects services as defined in the Kubernetes API on each node and can do simple
TCP, UDP, and SCTP stream forwarding or round robin TCP, UDP, and SCTP forwarding across a set of backends.
Service cluster IPs and ports are currently found through Docker-links-compatible
environment variables specifying ports opened by the service proxy. There is an optional
addon that provides cluster DNS for these cluster IPs. The user must create a service
with the apiserver API to configure the proxy.`,
		Run: func(cmd *cobra.Command, args []string) {
			verflag.PrintAndExitIfRequested()
			utilflag.PrintFlags(cmd.Flags())

			if err := initForOS(opts.WindowsService); err != nil {
				klog.Fatalf("failed OS init: %v", err)
			}

			if err := opts.Complete(); err != nil {
				klog.Fatalf("failed complete: %v", err)
			}
			if err := opts.Validate(args); err != nil {
				klog.Fatalf("failed validate: %v", err)
			}
			klog.Fatal(opts.Run())
		},
	}

	var err error
	opts.config, err = opts.ApplyDefaults(opts.config)
	if err != nil {
		klog.Fatalf("unable to create flag defaults: %v", err)
	}

	opts.AddFlags(cmd.Flags())

	// TODO handle error
	cmd.MarkFlagFilename("config", "yaml", "yml", "json")

	return cmd
}



kubernetes网络代理在每个节点上运行。这个

在每个节点上反映Kubernetes API中定义的服务，并且可以做简单的
TCP、UDP和SCTP流转发或通过一组后端进行循环TCP、UDP和SCTP转发。
服务集群IP和端口当前通过兼容Docker链接找到
指定由服务代理打开的端口的环境变量。有一个可选的
为这些群集IP提供群集DNS的加载项。用户必须创建服务
使用apiserver api配置代理


const (
	proxyModeUserspace   = "userspace"
	proxyModeIPTables    = "iptables"
	proxyModeIPVS        = "ipvs"
	proxyModeKernelspace = "kernelspace"
)



cat /etc/kubernetes/kube-proxy.config.yaml 
apiVersion: kubeproxy.config.k8s.io/v1alpha1
bindAddress: 192.168.10.242
clientConnection:
  kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig
clusterCIDR: 172.30.0.0/16
healthzBindAddress: 192.168.10.242:10256
hostnameOverride: 192.168.10.242
kind: KubeProxyConfiguration
metricsBindAddress: 192.168.10.242:10249
mode: "ipvs"




lsmod | grep ip_vs
ip_vs_sh               12688  0 
ip_vs_wrr              12697  0 
ip_vs_rr               12600  19 
ip_vs                 145497  25 ip_vs_rr,ip_vs_sh,ip_vs_wrr
nf_conntrack          133095  7 ip_vs,nf_nat,nf_nat_ipv4,xt_conntrack,nf_nat_masquerade_ipv4,nf_conntrack_netlink,nf_conntrack_ipv4
libcrc32c              12644  5 xfs,ip_vs,libceph,nf_nat,nf_conntrack


https://www.kubernetes.org.cn/3025.html




启用 ipvs 后与 1.7 版本的配置差异如下：

增加 –feature-gates=SupportIPVSProxyMode=true 选项，用于告诉 kube-proxy 开启 ipvs 支持，因为目前 ipvs 并未稳定
增加 ipvs-min-sync-period、–ipvs-sync-period、–ipvs-scheduler 三个参数用于调整 ipvs，具体参数值请自行查阅 ipvs 文档
增加 –masquerade-all 选项，以确保反向流量通过


masquerade-all 选项: 

kube-proxy ipvs 是基于 NAT 实现的，当创建一个 service 后，
kubernetes 会在每个节点上创建一个网卡，同时帮你将 Service IP(VIP) 绑定上，
此时相当于每个 Node 都是一个 ds，
而其他任何 Node 上的 Pod，
甚至是宿主机服务(比如 kube-apiserver 的 6443)都可能成为 rs；
按照正常的 lvs nat 模型，所有 rs 应该将 ds 设置成为默认网关，以便数据包在返回时能被 ds 正确修改；
在 kubernetes 将 vip 设置到每个 Node 后，默认路由显然不可行，所以要设置 –masquerade-all 选项，以便反向数据包能通过















