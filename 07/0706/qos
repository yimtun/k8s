request cpu 1 核心  mem 2G

limit  2核心 mem 4G

从调度来说已经把资源减半了  这部分差额可以供其他资源调度使用
在真正使用的时候 还是可以使用到 这个能力的 (limit)


只要request 没有 unallocated  (未划分的)就可以超卖

只要request 没有达到100%  就还可以再被调度资源

limit-pod 仅仅指定了 limit 所以request 和limit 是一样 

kubectl describe node 192.168.10.243
Capacity:
 cpu:                4
 ephemeral-storage:  36805060Ki
 hugepages-1Gi:      0
 hugepages-2Mi:      0
 memory:             7992344Ki
 pods:               110

Non-terminated Pods:         (3 in total)
  Namespace                  Name                                  CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ---------                  ----                                  ------------  ----------  ---------------  -------------
  default                    limit-pod                             200m (5%)     200m (5%)   20Mi (0%)        20Mi (0%)
  kube-system                heapster-77db4c4496-czj75             0 (0%)        0 (0%)      0 (0%)           0 (0%)
  kube-system                monitoring-grafana-5c5fbc85b-nzlcl    0 (0%)        0 (0%)      0 (0%)           0 (0%)






request  总是是  100%
limits  可以超过100%



QOS 驱逐策略
宿主超卖 当宿主资源紧张的时候 cpu mem 打满的时候  驱逐策略  按照一个级别进行驱逐


可以使用 LimitRange 对象来为每个pod 都设置上默认的 requests 和 limits

基于ns 的资源配额
每个业务线可以按中重要级别 定义为是几级的业务   1 级最重要  2级次要  3级最次要 类似


pr    
pod-extend-resources  gitbook




扩展资源  
某个资源在宿主机上没有上报到 api server
但是想要基于扩展资源 做调度 使用request  limit 的 限制

ex1 将磁盘资源多为一种扩展资源  当做一种调度的依据


[root@master01 ~]# kubectl proxy
Starting to serve on 127.0.0.1:8001



kubectl proxy  


cat > disk.sh  << EOF
#!/bin/bash
curl -XPATCH http://127.0.0.1:8001/api/v1/nodes/\$1/status -H "Accept: application/json" -H "Content-Type: application/json-patch+json"  -d '
[
{
    "op": "add",
    "path": "/status/capacity/reboot.kubernetes.com~1volume",
    "value": "2700"
}
]'
EOF


sh  disk.sh 192.168.10.243


kubectl  describe  no 192.168.10.243 
Capacity:
 cpu:                           4
 ephemeral-storage:             36805060Ki
 hugepages-1Gi:                 0
 hugepages-2Mi:                 0
 memory:                        7992344Ki
 pods:                          110
 reboot.kubernetes.com/volume:  2700



cat  > disk-pod.yaml  << EOF
apiVersion: v1
kind: Pod
metadata:
  name: volume1
spec:
  containers:
  - name: volume1
    image: nginx
    imagePullPolicy: IfNotPresent
    resources:
      limits:
        cpu: "2"
        reboot.kubernetes.com/volume: 300
        memory: 4Gi
      requests:
        cpu: "1"
        reboot.kubernetes.com/volume: 300
        memory: "2Gi"
EOF


kubectl  create  -f disk-pod.yaml 
pod/volume1 created


被调度到 拥有disk资源的243 上
kubectl  get pod volume1  -o wide
NAME      READY   STATUS    RESTARTS   AGE   IP            NODE             NOMINATED NODE
volume1   1/1     Running   0          24s   172.30.38.5   192.168.10.243   <none>




kubectl  describe  node 192.168.10.243
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource                      Requests      Limits
  --------                      --------      ------
  cpu                           1200m (30%)   2200m (55%)
  memory                        2068Mi (26%)  4116Mi (53%)
  reboot.kubernetes.com/volume  300           300


volume 扩展资源的 request  limit 都是300     没有超卖
CPU MEM  放水了

Non-terminated Pods:         (4 in total)
  Namespace                  Name                                  CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ---------                  ----                                  ------------  ----------  ---------------  -------------
  default                    limit-pod                             200m (5%)     200m (5%)   20Mi (0%)        20Mi (0%)
  default                    volume1                               1 (25%)       2 (50%)     2Gi (26%)        4Gi (53%)
  kube-system                heapster-77db4c4496-czj75             0 (0%)        0 (0%)      0 (0%)           0 (0%)
  kube-system                monitoring-grafana-5c5fbc85b-nzlcl    0 (0%)        0 (0%)      0 (0%)           0 (0%)


volume1  申请的事两核心 4G  但是实际  调度的时候 是1核心2G



